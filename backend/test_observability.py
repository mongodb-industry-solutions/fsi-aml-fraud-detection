#!/usr/bin/env python3
"""
Test script for Azure AI Foundry Agent Observability System
Tests all observability components and integration
"""

import asyncio
import os
import sys
import logging
from datetime import datetime
from typing import Dict, Any

# Add the backend directory to Python path
sys.path.insert(0, '/Users/mehar.grewal/Desktop/Work/Coding/Finance/Fraud Detection Demo Mar 25/fsi-aml-fraud-detection/backend')

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_observability_imports():
    """Test that all observability components can be imported"""
    logger.info("üß™ Testing observability imports...")
    
    try:
        from azure_foundry.observability import (
            AgentObserver, DecisionTracker, DecisionType, ConfidenceLevel,
            configure_observability, get_tracer, create_span,
            RunStepInspector, RunInspectionResult, DecisionFactor, AgentDecision
        )
        logger.info("‚úÖ All observability components imported successfully")
        return True
    except ImportError as e:
        logger.error(f"‚ùå Import error: {e}")
        return False

async def test_telemetry_configuration():
    """Test OpenTelemetry configuration"""
    logger.info("üß™ Testing telemetry configuration...")
    
    try:
        from azure_foundry.observability import configure_observability
        
        # Test basic configuration without Azure Monitor
        success = configure_observability(
            connection_string=None,
            enable_console_export=True,
            service_name="test-service"
        )
        
        if success:
            logger.info("‚úÖ Telemetry configuration successful")
            return True
        else:
            logger.warning("‚ö†Ô∏è Telemetry configuration returned False (dependencies may be missing)")
            return True  # Still considered success for testing
    except Exception as e:
        logger.error(f"‚ùå Telemetry configuration error: {e}")
        return False

async def test_decision_tracker():
    """Test DecisionTracker functionality"""
    logger.info("üß™ Testing DecisionTracker...")
    
    try:
        from azure_foundry.observability import (
            DecisionTracker, DecisionType, ConfidenceLevel, DecisionFactor
        )
        
        tracker = DecisionTracker()
        
        # Test basic decision tracking
        decision_id = tracker.track_decision(
            thread_id="test_thread_123",
            run_id="test_run_456",
            agent_id="test_agent_789",
            decision_type=DecisionType.FRAUD_ASSESSMENT,
            decision_summary="Test fraud assessment decision",
            confidence_level=ConfidenceLevel.HIGH,
            confidence_score=0.85,
            input_context={"test_transaction": {"amount": 1500.0}},
            reasoning_chain=["Analyzed patterns", "Applied risk model"],
            tools_used=["analyze_transaction_patterns"],
            processing_time_ms=1200.0
        )
        
        logger.info(f"‚úÖ Decision tracked with ID: {decision_id}")
        
        # Test fraud assessment decision tracking
        fraud_decision_id = tracker.track_fraud_assessment_decision(
            thread_id="test_thread_123",
            run_id="test_run_789",
            agent_id="test_agent_789",
            transaction_data={"amount": 2000.0, "merchant": "Test Store"},
            risk_score=75.5,
            fraud_indicators=["velocity_exceeded", "new_merchant"],
            reasoning=["High velocity detected", "Unknown merchant"],
            tools_used=["check_sanctions_lists"],
            processing_time_ms=850.0
        )
        
        logger.info(f"‚úÖ Fraud decision tracked with ID: {fraud_decision_id}")
        
        # Test analytics
        analytics = tracker.get_decision_analytics("test_agent_789")
        logger.info(f"‚úÖ Decision analytics: {analytics['total_decisions']} decisions tracked")
        
        # Test data export
        export_file = tracker.export_decision_history(
            file_path="test_decisions.json",
            agent_id="test_agent_789"
        )
        logger.info(f"‚úÖ Decision history exported to: {export_file}")
        
        return True
    except Exception as e:
        logger.error(f"‚ùå DecisionTracker test error: {e}")
        return False

async def test_agent_observer():
    """Test AgentObserver functionality"""
    logger.info("üß™ Testing AgentObserver...")
    
    try:
        from azure_foundry.observability import AgentObserver
        
        # Create a mock agents client for testing
        class MockAgentsClient:
            pass
        
        mock_client = MockAgentsClient()
        observer = AgentObserver(mock_client)
        
        # Test conversation monitoring start
        context = await observer.start_conversation_monitoring(
            thread_id="test_thread_123",
            agent_id="test_agent_789",
            context={"test_context": "fraud_analysis"}
        )
        
        logger.info(f"‚úÖ Conversation monitoring started for thread: {context.thread_id}")
        
        # Test metrics retrieval
        metrics = observer.get_agent_metrics("test_agent_789")
        logger.info(f"‚úÖ Agent metrics retrieved: {metrics}")
        
        # Test conversation end
        await observer.end_conversation_monitoring("test_thread_123", "completed")
        logger.info("‚úÖ Conversation monitoring ended")
        
        # Test metrics export
        export_file = observer.export_metrics_report("test_metrics.json")
        logger.info(f"‚úÖ Metrics exported to: {export_file}")
        
        return True
    except Exception as e:
        logger.error(f"‚ùå AgentObserver test error: {e}")
        return False

async def test_run_inspector():
    """Test RunStepInspector functionality"""
    logger.info("üß™ Testing RunStepInspector...")
    
    try:
        from azure_foundry.observability import RunStepInspector, RunStepDetail, ToolCallDetail
        
        # Create a mock agents client
        class MockAgentsClient:
            pass
        
        mock_client = MockAgentsClient()
        inspector = RunStepInspector(mock_client)
        
        logger.info("‚úÖ RunStepInspector initialized successfully")
        
        # Test data classes
        tool_call = ToolCallDetail(
            id="tool_123",
            type="function",
            name="test_function",
            arguments='{"param": "value"}',
            status="completed",
            execution_time_ms=500.0
        )
        
        run_step = RunStepDetail(
            step_id="step_123",
            step_type="tool_calls",
            status="completed",
            created_at=datetime.now().isoformat(),
            tool_calls=[tool_call],
            execution_time_ms=600.0
        )
        
        logger.info("‚úÖ RunStepDetail and ToolCallDetail created successfully")
        
        return True
    except Exception as e:
        logger.error(f"‚ùå RunStepInspector test error: {e}")
        return False

async def test_conversation_handler_integration():
    """Test observability integration with NativeConversationHandler"""
    logger.info("üß™ Testing NativeConversationHandler with observability...")
    
    try:
        from azure_foundry.conversation.native_conversation import NativeConversationHandler
        
        # Create a mock agents client
        class MockAgentsClient:
            pass
        
        mock_client = MockAgentsClient()
        
        # Test handler initialization with observability
        handler = NativeConversationHandler(
            agents_client=mock_client,
            agent_id="test_agent_789",
            enable_observability=True,
            observability_config={
                "configure_telemetry": False,  # Skip telemetry setup for testing
                "enable_console_export": True
            }
        )
        
        logger.info("‚úÖ NativeConversationHandler with observability initialized")
        
        # Test metrics retrieval
        metrics = handler.get_observability_metrics()
        logger.info(f"‚úÖ Observability metrics: {metrics['observability_enabled']}")
        
        return True
    except Exception as e:
        logger.error(f"‚ùå NativeConversationHandler test error: {e}")
        return False

async def test_tracing_functionality():
    """Test OpenTelemetry tracing functionality"""
    logger.info("üß™ Testing OpenTelemetry tracing...")
    
    try:
        from azure_foundry.observability import create_span, get_tracer
        
        # Test tracer creation
        tracer = get_tracer("test-tracer")
        logger.info("‚úÖ Tracer created successfully")
        
        # Test span creation
        with create_span("test_operation", {"test_attr": "test_value"}) as span:
            if hasattr(span, 'set_attribute'):
                span.set_attribute("operation_success", True)
            logger.info("‚úÖ Span created and used successfully")
        
        return True
    except Exception as e:
        logger.error(f"‚ùå Tracing test error: {e}")
        return False

async def cleanup_test_files():
    """Clean up test files created during testing"""
    logger.info("üß™ Cleaning up test files...")
    
    test_files = [
        "test_decisions.json",
        "test_metrics.json"
    ]
    
    for file_path in test_files:
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                logger.info(f"‚úÖ Removed test file: {file_path}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not remove {file_path}: {e}")

async def run_all_tests():
    """Run all observability tests"""
    logger.info("üöÄ Starting Azure AI Foundry Observability System Tests")
    logger.info("=" * 60)
    
    tests = [
        ("Import Tests", test_observability_imports),
        ("Telemetry Configuration", test_telemetry_configuration),
        ("DecisionTracker", test_decision_tracker),
        ("AgentObserver", test_agent_observer),
        ("RunStepInspector", test_run_inspector),
        ("Conversation Handler Integration", test_conversation_handler_integration),
        ("Tracing Functionality", test_tracing_functionality)
    ]
    
    results = []
    for test_name, test_func in tests:
        logger.info(f"\nüîç Running {test_name}...")
        try:
            result = await test_func()
            results.append((test_name, result))
            status = "‚úÖ PASSED" if result else "‚ùå FAILED"
            logger.info(f"{status}: {test_name}")
        except Exception as e:
            logger.error(f"‚ùå FAILED: {test_name} - {e}")
            results.append((test_name, False))
    
    # Cleanup
    await cleanup_test_files()
    
    # Summary
    logger.info("\n" + "=" * 60)
    logger.info("üìä TEST RESULTS SUMMARY")
    logger.info("=" * 60)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        logger.info(f"{status}: {test_name}")
    
    logger.info(f"\nüéØ Overall Result: {passed}/{total} tests passed")
    
    if passed == total:
        logger.info("üéâ All tests passed! Observability system is working correctly.")
    else:
        logger.warning(f"‚ö†Ô∏è {total - passed} test(s) failed. Please review the errors above.")
    
    return passed == total

if __name__ == "__main__":
    success = asyncio.run(run_all_tests())
    sys.exit(0 if success else 1)